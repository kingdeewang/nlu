# ! -*- coding: utf-8 -*-import osimport pickleimport pandas as pdfrom margin_softmax import *from keras.layers import *from flask import Flaskimport requestsimport jsonfrom bs4 import BeautifulSoupfrom keras.models import Modelfrom vib import VIBfrom keras.constraints import unit_normimport osimport loggingfrom tqdm import tqdmfrom urllib.parse import quotefrom flask.json import jsonifyfrom sys import version_infoimport urllibimport randomimport reimport jsonfrom elasticsearch import Elasticsearchfrom kgqa.chatbot_graph import ChatBotGraph from util import utility, utility_mysqllogging.basicConfig(filename='/home/v-rogenhxh/deeplearning/nlu/log/log.txt', level=logging.INFO)def putenv(key, value):        environ = utility.pickle_obj('gunicorn.ini')    environ[key] = value    utility.pickle_obj('gunicorn.ini', environ)        def getenv(key = None):    environ = utility.pickle_obj('gunicorn.ini')    if key is not None:        return environ[key]    return environ    app = Flask(__name__)model = Noneencoder = Nonechars = {}id2char = {}char2id = {}min_count = 2maxlen = 30word_size = 128handler = ChatBotGraph()data = pd.read_csv('data/tongyiju.csv', encoding='utf-8', header=None, delimiter='\t')es = Elasticsearch(hosts=[{"host": "localhost", "port": 9200}]) def strQ2B(ustring):  # 全角转半角    rstring = ''    for uchar in ustring:        inside_code = ord(uchar)        if inside_code == 12288:  # 全角空格直接转换            inside_code = 32        elif (inside_code >= 65281 and inside_code <= 65374):  # 全角字符（除空格）根据关系转化            inside_code -= 65248        rstring += chr(inside_code)    return rstringdata[1] = data[1].apply(strQ2B)data[1] = data[1].str.lower()chars = {}for s in tqdm(iter(data[1])):    for c in s:        if c not in chars:            chars[c] = 0        chars[c] += 1chars = {i:j for i, j in chars.items() if j >= min_count}def getRecords(query,source=None):    global es    if source is not None:        body = {            "query": {                    "bool":{                        "must":[                            {                                "term":{                                    "question":query                                },                                "term":{                                    "source":source                                }                            }                        ]                    }            }        }        result = es.search(index="chat", doc_type="_doc", body=body)    else:        result = es.search(index="chat", doc_type="_doc",q="question:"+query)    if result is None:        return result    return result['hits']['hits']def string2id(s):    _ = [char2id.get(i, 1) for i in s[:maxlen]]    _ = _ + [0] * (maxlen - len(_))    return _# 定义文本相似度def most_similar(ss,source=None):    global encoder    datas = getRecords(ss,source)    if datas is None or len(datas) == 0:        return ""        keywords_vec = []    dict = {}    answer = []    for data in datas:        data=data['_source']        if not 'question' in data:            continue                keyword = data['question']        dict = {}        dict['question'] = keyword        if 'answer' in data:            content = data['answer']            if type(content).__name__ == 'list':                content = content[0]            dict['answer'] = content        else:            content = data['content']            if type(content).__name__ == 'list':                content = content[0]            dict['answer'] = content        if 'category' in data:            dict['category'] = data['category']            keywords_vec.append(string2id(keyword))        answer.append(dict)    if encoder is None:        x_in = Input(shape=(maxlen,))        x_embedded = Embedding(len(chars) + 2,                       word_size)(x_in)        x = GRU(word_size)(x_embedded)        x = Lambda(lambda x: K.l2_normalize(x, 1))(x)        encoder = Model(x_in, x)  # 最终的目的是要得到一个编码器        encoder.load_weights("gru_data/sim_encoder_weight.h5")        valid_vec = encoder.predict(np.array(list(keywords_vec)),                           verbose=True,                           batch_size=100)  # encoder计算句向量    v = encoder.predict(np.array([string2id(ss)]))[0]    sims = np.dot(valid_vec, v)    # for i in range(len(sims)):    #    logging.info("%s,%s", answer[i]['keywords'],sims[i])        for i in range(len(sims)):        if sims[i] > 0.91 :            dict = answer[i]            return json.dumps(dict)    return ''    @app.route('/getAnswer/<sentence>')def getAnswer(sentence):    content = most_similar(sentence)    return content@app.route('/getCustomAnswer/<sentence>/<source>')def getCustomAnswer(sentence, source):    content = most_similar(sentence, source)    return content@app.route('/getKgqaAnswer/<sentence>')def getKgqaAnswer(sentence):    global handler    if handler is None:        handler = ChatBotGraph()    return handler.chat_main(sentence)@app.route('/domain/<sentence>')def show_sentence_domain(sentence):    global model    global char2id    global encoder    global label    if model is None:        if(len(char2id) == 0):            x_train, y_train, char2id, combined, label = pickle.load(open('gru_data/data.config', 'rb'))        # print(model.to_json())  #秒级时间戳            # 正式模型，基于GRU的分类器        x_in = Input(shape=(maxlen,))        x_embedded = Embedding(len(char2id) + 2,                               word_size, mask_zero=True)(x_in)        x = Bidirectional(GRU(word_size))(x_embedded)        # x = Bidirectional(GRU(word_size))(x)        # x_mask = Lambda(lambda x: K.cast(K.greater(K.expand_dims(x, 2), 0), 'float32'))(x)        # x= AttentionLayer()(x)        # encoder = Model(x_in, x) # 最终的目的是要得到一个编码器        z_mean = Dense(128)(x)        z_log_var = Dense(128)(x)        x = VIB(0.1)([z_mean, z_log_var])        x = Lambda(lambda x: K.l2_normalize(x, 1))(x)        pred = Dense(len(label),                     use_bias=False,                     kernel_constraint=unit_norm())(x)        model = Model(x_in, pred)  # 用分类问题做训练        model.load_weights("gru_data/classfication_weight.h5")    x_predict = np.array([string2id(sentence)])    predict = model.predict(x_predict)    logging.info("%s,%s %f" % (sentence, label[np.argmax(predict)], predict[0][np.argmax(predict)]))    threshold = 0.50    if(label[np.argmax(predict)] == 'recall' or label[np.argmax(predict)] == 'error'):        if len(sentence) < 15:            threshold = 0.7        else:            threshold = 0.6    if(predict[0][np.argmax(predict)] >= threshold):        return label[np.argmax(predict)]    return ""def get_classification_model():    global model    global char2id    global encoder    global label        if(len(char2id) == 0):        _, _, char2id, _, label = pickle.load(open('gru_data/data.config', 'rb'))    # print(model.to_json())  #秒级时间戳        # 正式模型，基于GRU的分类器    x_in = Input(shape=(maxlen,))    x_embedded = Embedding(len(char2id) + 2,                           word_size, mask_zero=True)(x_in)    x = Bidirectional(GRU(word_size))(x_embedded)    # x = Bidirectional(GRU(word_size))(x)    # x_mask = Lambda(lambda x: K.cast(K.greater(K.expand_dims(x, 2), 0), 'float32'))(x)    # x= AttentionLayer()(x)    # encoder = Model(x_in, x) # 最终的目的是要得到一个编码器    z_mean = Dense(128)(x)    z_log_var = Dense(128)(x)    x = VIB(0.1)([z_mean, z_log_var])    x = Lambda(lambda x: K.l2_normalize(x, 1))(x)    pred = Dense(len(label),                 use_bias=False,                 kernel_constraint=unit_norm())(x)    model = Model(x_in, pred)  # 用分类问题做训练    model.load_weights("gru_data/classfication_weight.h5")    @app.route('/getJSONService/<sentence>', methods=['POST', 'GET'])def get_service(sentence):    global model    global char2id    global encoder    global label    if model is None:        get_classification_model()    x_predict = np.array([string2id(sentence)])    predict = model.predict(x_predict)    retJSON = {"retcode":0};#     logging.info("%s,%s %f" % (sentence, label[np.argmax(predict)], predict[0][np.argmax(predict)]))#     threshold = 0.5#     if(label[np.argmax(predict)] == 'recall' or label[np.argmax(predict)] == 'error'):#         if len(sentence) < 15:#             threshold = 0.7#         else:#             threshold = 0.6    top_k = 2    top_k_idx = predict[0].argsort()[::-1][0:top_k]        # 如果两者值相差很小，则返回两个标签    retArray = []    weight = predict[0][top_k_idx[0]]    retJSON = {"retcode":0}    retJSON['category'] = label[top_k_idx[0]]    service = category2service(label[top_k_idx[0]])    if(isinstance(service,list)):        retJSON['service'] = service[0]        retJSON['code'] = service[1]    else:        retJSON['service'] = service    retJSON['weight'] = str(weight)    retJSON['text'] = sentence    retArray.append(retJSON)    retJSON1 = {"retcode":0}    retJSON1['category'] = label[top_k_idx[1]]    service = category2service(label[top_k_idx[1]])    if(isinstance(service,list)):        retJSON1['service'] = service[0]        retJSON1['code'] = service[1]    else:        retJSON1['service'] = service    weight = predict[0][top_k_idx[1]]    retJSON1['weight'] = str(weight)    retJSON1['text'] = sentence    retArray.append(retJSON1)    return jsonify(retArray)def category2service(category):    if '.' in category:        return category.split('.')    return category        # below is added by Cosmos:from flask import request    #from sequence import nerecognizerif __name__ == '__main__':    from werkzeug.contrib.fixers import ProxyFix    app.wsgi_app = ProxyFix(app.wsgi_app)    app.run()def get_category(sentence, context = None):    global model    global char2id    global encoder    global label        if model is None:        get_classification_model()        is_single = False    if isinstance(sentence, str):        is_single = True        if context:            sentence = '%s|%s' % (sentence, context)        sentence = [sentence]    x_predict = np.array([string2id(sent) for sent in sentence])    predict = model.predict(x_predict, batch_size=128)        category = [label[i] for i in np.argmax(predict, -1)]    service = [category2service(x) for x in category]    if is_single:        return service[0], category[0]    else:        return service, category    @app.route('/ner', methods=['POST', 'GET'])def ner():        service = request.args.get('service')    if service is None:                service = request.form.get('service')            text = request.args.get('text')    if text is None:        text = urllib.parse.unquote(request.form.get('text'))        code = None    if service is None:        category = show_sentence_domain(text)        print('category =', category)        if '.' in category:            service, code = category.split('.')        else:            service = category    print('%s = %s' % (text, service))        dic = {}    dic['text'] = text    dic['service'] = service        dic['semantic'] = semantic = nerecognizer.predict(service, text)                    intent = semantic['intent']    if 'code' in intent:        del intent['code']        if code is not None:        dic['code'] = code            return jsonify(dic)@app.route('/eval', methods=['POST', 'GET'])def evaluate():    decode = request.args.get('decode')    if decode is None:                decode = request.form.get('decode')            python = request.args.get('python')    if python is None:                python = request.form.get('python')            if decode:        print('python code before decoding:')        print(python)        python = urllib.parse.unquote(python)            print('python code:')        print(python)    print('decode = ', decode)        result = eval(python)        print('result =', result)    if isinstance(result, set):#         TypeError: Object of type 'set' is not JSON serializable        result = [*result]    return jsonify(result)@app.route('/restart', methods=['GET', 'POST'])def restart():    print('restart all threads')        url_root = request.url_root        utility.restart(utility.get_tcp_from_url_root(url_root), 0)    return 'restart is executed'def training_service_epoch(epoch):    import classficatiton_vib_mysql    classficatiton_vib_mysql.training(epoch)from flask import g@app.route('/update/repertoire', methods=['GET', 'POST'])def update_repertoire():    print("os.environ['workers'] = ", os.environ['workers'])        service = request.args.get('service')    if service is None:                service = request.form.get('service')            slot = request.args.get('slot')    if slot is None:                slot = request.form.get('slot')    text = request.args.get('text')    if text is None:                text = request.form.get('text')        if not hasattr(g._get_current_object(), 'repertoire'):                g.repertoire = {}        if not service in g.repertoire:        g.repertoire[service] = {}            g.repertoire[service][text] = slot            return 'update_repertoire'#from sequence.globals_variables import *#from sequence.nerecognizer import slotSelector