# ! -*- coding: utf-8 -*-import osimport pickleimport pandas as pdfrom keras.layers import *from margin_softmax import *from flask import Flaskimport requestsimport jsonfrom bs4 import BeautifulSoupfrom keras.models import Modelfrom vib import VIBfrom keras.constraints import unit_normimport osfrom urllib.parse import quotefrom flask.json import jsonifyfrom sys import version_infoimport urllibimport randomimport reimport jsonfrom util import utility, utility_mysql# os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"   # see issue #152# os.environ["CUDA_VISIBLE_DEVICES"] = ""def putenv(key, value):    environ = utility.pickle_obj('gunicorn.ini')    environ[key] = value    utility.pickle_obj('gunicorn.ini', environ)def getenv(key=None):    environ = utility.pickle_obj('gunicorn.ini')    if key is not None:        return environ[key]    return environapp = Flask(__name__)# print("getenv = ", getenv())def string2id(s):    global char2id    _ = [char2id.get(i, 1) for i in s[:maxlen]]    _ = _ + [0] * (maxlen - len(_))    return _@app.route('/domain/<sentence>')def show_sentence_domain(sentence):    global model    global label    if model is None:        get_classification_model()    x_predict = np.array([string2id(sentence)])    predict = model.predict(x_predict)    return label[np.argmax(predict)]def get_classification_model():    global char2id    global label    global model    if len(char2id) == 0:        _, _, char2id, _, label = pickle.load(open('gru_data/data.config', 'rb'))    x_in = Input(shape=(maxlen,))    x_embedded = Embedding(len(char2id) + 2,                           word_size, mask_zero=True)(x_in)    x = Bidirectional(GRU(word_size))(x_embedded)    z_mean = Dense(128)(x)    z_log_var = Dense(128)(x)    x = VIB(0.1)([z_mean, z_log_var])    x = Lambda(lambda x: K.l2_normalize(x, 1))(x)    pred = Dense(len(label),                 use_bias=False,                 kernel_constraint=unit_norm())(x)    model = Model(x_in, pred)    model.load_weights("gru_data/classfication_weight.h5")encoder = Nonechars = {}id2char = {}char2id = {}min_count = 2maxlen = 30word_size = 128data = pd.read_csv('data/tongyiju.csv', encoding='utf-8', header=None, delimiter='\t')def strQ2B(ustring):  # 全角转半角    rstring = ''    for uchar in ustring:        inside_code = ord(uchar)        if inside_code == 12288:  # 全角空格直接转换            inside_code = 32        elif (inside_code >= 65281 and inside_code <= 65374):  # 全角字符（除空格）根据关系转化            inside_code -= 65248        rstring += chr(inside_code)    return rstringdata[1] = data[1].apply(strQ2B)data[1] = data[1].str.lower()chars = {}for s in data[1]:    for c in s:        if c not in chars:            chars[c] = 0        chars[c] += 1chars = {i:j for i, j in chars.items() if j >= min_count}model = Nonedef getRecords(url):    session = requests.session()    session.keep_alive = False    headers = {        'Host': 'jandan.net',        'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:53.0) Gecko/20100101 Firefox/53.0',        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',         'Accept-Language': 'zh-CN,zh;q=0.8,en-US;q=0.5,en;q=0.3',         'Accept-Encoding': 'gzip, deflate',    }    wb_data_sub = requests.get(url)    soup = BeautifulSoup(wb_data_sub.text, 'lxml')    data = soup.p.string    text = json.loads(data)    records = text['response']['docs']    return records# 定义文本相似度def most_similar(ss):    global encoder    p = re.compile(r'[$()#+&*]')    ss = re.sub(p, "", ss)    ss = ss.replace(u"？", "")    tt = quote(ss.encode('utf-8'))    url = "http://localhost:8983/solr/semantic/select?q=" + tt + "&wt=json"    datas = getRecords(url)    if(len(datas) == 0):        return ""    keywords_vec = []    dict = {}    answer = []    for data in datas:        if not 'keywords' in data:            continue        tt = data['keywords'].split(";")        for keyword in tt:            dict = {}            dict['keywords'] = keyword            if 'content' in data:                content = data['content']                if type(content).__name__ == 'list':                    content = content[0]                dict['answer'] = content            else:                dict['answer'] = ''            if 'category' in data:                dict['category'] = data['category']            else:                continue        keywords_vec.append(string2id(keyword))        answer.append(dict)    if encoder is None:        x_in = Input(shape=(maxlen,))        x_embedded = Embedding(len(chars) + 2,                       word_size)(x_in)        x = GRU(word_size)(x_embedded)        x = Lambda(lambda x: K.l2_normalize(x, 1))(x)        encoder = Model(x_in, x)  # 最终的目的是要得到一个编码器        encoder.load_weights("gru_data/sim_encoder_weight.h5")    valid_vec = encoder.predict(np.array(list(keywords_vec)),                           verbose=True,                           batch_size=100)  # encoder计算句向量    v = encoder.predict(np.array([string2id(ss)]))[0]    sims = np.dot(valid_vec, v)    for i in range(len(sims)):        if sims[i] > 0.91 :            dict = answer[i]            return json.dumps(dict)    return ''@app.route('/getAnswer/<sentence>')def show_sentence_domain1(sentence):    content = most_similar(sentence)    return content@app.route('/getJSONService/<sentence>', methods=['POST', 'GET'])def get_service(sentence):    global model    global char2id    global encoder    global label    if model is None:        get_classification_model()    x_predict = np.array([string2id(sentence)])    predict = model.predict(x_predict)    retJSON = {"retcode":0};    top_k = 2    top_k_idx = predict[0].argsort()[::-1][0:top_k]    # 如果两者值相差很小，则返回两个标签    if predict[0][top_k_idx[0]] > 0.3 and predict[0][top_k_idx[0]] - predict[0][top_k_idx[1]] < 0.1:        retArray = []        weight = predict[0][top_k_idx[0]] / (predict[0][top_k_idx[0]] + predict[0][top_k_idx[1]])        retJSON = {"retcode":0}        retJSON['category'] = label[top_k_idx[0]]        retJSON['service'] = category2service(label[top_k_idx[0]])        retJSON['weight'] = str(weight)#         retJSON['text'] = sentence        retArray.append(retJSON)        retJSON1 = {"retcode":0}        weight = 1 - weight        retJSON['category'] = label[top_k_idx[1]]        retJSON1['service'] = category2service(label[top_k_idx[1]])        retJSON1['weight'] = str(weight)#         retJSON1['text'] = sentence        retArray.append(retJSON1)        return jsonify(retArray)#     if predict[0][np.argmax(predict)] > threshold:    service = label[np.argmax(predict)]    code = ''    source = ''    type = ''    apptype = 0    retJSON['category'] = service    if service == 'poi' or service == 'navi':        code = 'NAVI'    elif service == 'costtime':        code = 'TIME'    elif service == 'distance':        code = 'DISTANCE'    elif service == 'location' or service == "postion":        code = 'LOCATION'    elif service == 'traffic':        code = 'TRAFFIC'    if service == 'tag' or service == 'genre':        code = 'SEARCH_TAG'    elif service == 'billboard':        code = 'SEARCH_BILLBORAD'    elif service == 'recommend':        code = 'SEARCH_RECOMMEND'    elif service == 'play_card':        source = 'tcard'    elif service == 'red_music':        type = 'redheart'    elif service == 'play_local':        type = 'local'    elif service == 'red_music':        type = 'redheart'    elif service == 'play_random':        type = 'local'    elif service == 'qq_music':        apptype = 1    retJSON['service'] = category2service(service)    retJSON['code'] = code    if source != '':        retJSON['source'] = source    if type != '':        retJSON['type'] = source    if apptype != 0:        retJSON['apptype'] = apptype    return jsonify(retJSON)def category2service(category):    if '.' in category:        return category.split('.')[0]    return categoryfrom flask import requestdef get_category(sentence, context=None):    global model    global char2id    global encoder    global label    is_single = False    if isinstance(sentence, str):        is_single = True        if context:            sentence = '%s|%s' % (sentence, context)        sentence = [sentence]    x_predict = np.array([string2id(sent) for sent in sentence])    predict = model.predict(x_predict, batch_size=128)    category = [label[i] for i in np.argmax(predict, -1)]    service = [category2service(x) for x in category]    if is_single:        return service[0], category[0]    else:        return service, category@app.route('/service', methods=['POST', 'GET'])def service():    text = request.args.get('text') or request.form.get('text')    print('text =', text)    return show_sentence_domain(text)# http://127.0.0.1:8000/_service?text=播放下一站下一站传奇城乘风破浪@app.route('/_service', methods=['POST', 'GET'])def _service():    if model is None:        get_classification_model()    if not hasattr(model, 'wEmbedding'):        binary_file = utility.modelsDirectory + 'cn/gru_data/service.bin'        if not os.path.exists(binary_file):            with open(binary_file, 'wb') as file:                utility.Embedding(char2id, *model.get_layer(index=1).get_weights()).write(file)                utility.BiGRU(model.get_layer(index=2).get_weights(), 'concat').write(file)                utility.Dense(*model.get_layer(index=3).get_weights()).write(file)                utility.Dense(*model.get_layer(index=7).get_weights()).write(file)        with open(binary_file, 'rb') as file:            model.wEmbedding = utility.Embedding.read(file)            model.wGRU = utility.BiGRU.read(file, 'concat')            model.wDense_mean = utility.Dense.read(file)            model.wDense_pred = utility.Dense.read(file, False)    if request.args.get('label') or request.form.get('label'):        return jsonify(label)    text = request.args.get('text') or request.form.get('text')    debug = request.args.get('debug') or request.form.get('debug')    if debug:        result = []        x = model.wEmbedding.call(text, False, maxlen);        result.append([*x[0]])        result.append([*x[-1]])        x = model.wGRU.call(x, debug=result)        result.append([*x])        x = model.wDense_mean.call(x)        result.append([*x])        x = utility.l2_normalize(x)        result.append([*x])        y = model.wDense_pred.call(x)        result.append([*y])        return jsonify(result)    else:        x = model.wEmbedding.call(text, False, maxlen);        x = model.wGRU.call(x)        x = model.wDense_mean.call(x)        x = utility.l2_normalize(x)        y = model.wDense_pred.call(x)        return label[y.argmax()]@app.route('/eval', methods=['POST', 'GET'])def evaluate():    decode = request.args.get('decode')    if decode is None:        decode = request.form.get('decode')    python = request.args.get('python')    if python is None:        python = request.form.get('python')    if decode:        print('python code before decoding:')        print(python)        python = urllib.parse.unquote(python)    print('python code:')    print(python)    print('decode = ', decode)    result = eval(python)    print('result =', result)    if isinstance(result, set):#         TypeError: Object of type 'set' is not JSON serializable        result = [*result]    return jsonify(result)@app.route('/restart', methods=['GET', 'POST'])def restart():    print('restart all threads')    url_root = request.url_root    utility.restart(utility.get_tcp_from_url_root(url_root), 0)    return 'restart is executed'def training_service_epoch(epoch):    import classficatiton_vib_mysql    classficatiton_vib_mysql.training(epoch)# import coachif __name__ == '__main__':    from werkzeug.contrib.fixers import ProxyFix#     print('app.config:')#     print(app.config)    app.wsgi_app = ProxyFix(app.wsgi_app)    app.run(port=8000, threaded=False)